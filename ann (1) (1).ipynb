{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0npWYMJnFBa",
    "outputId": "2b9677f6-9a0c-49f2-e44d-605d66335917"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "KNSMQMs_nQ6T"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ezQ5ym0NoZrY",
    "outputId": "b1fe508e-c66d-4dbf-8611-9bc48833ba24"
   },
   "outputs": [],
   "source": [
    "# #cats = r'\\D:\\downloads\\Module 3_ Convolutional Neural Networks\\Data set, Presentation and Python Notebook for Image recognition project module\\train\\cats'\n",
    "# train_dir = r'D:\\downloads\\Module 3_ Convolutional Neural Networks\\Data set, Presentation and Python Notebook for Image recognition project module\\train'\n",
    "# #test_dir =r'\\Test'\n",
    "# #dogs =r'\\D:\\downloads\\Module 3_ Convolutional Neural Networks\\Data set, Presentation and Python Notebook for Image recognition project module\\train\\dogs' \n",
    "\n",
    "\n",
    "\n",
    "# # for i in cats:   \n",
    "# #     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb',\n",
    "# #     target_size= (28,28))\n",
    "# #     image=np.array(image)\n",
    "# #     data.append(image)\n",
    "# #     labels.append(0)\n",
    "# # for i in dogs:   \n",
    "# #     image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb',\n",
    "# #     target_size= (280,280))\n",
    "# #     image=np.array(image)\n",
    "# #     data.append(image)\n",
    "# #     labels.append(1)\n",
    "# # data = np.array(data)\n",
    "# # labels = np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_generator, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1600)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the directory containing the images\n",
    "data_dir = r'D:\\downloads\\Module 3_ Convolutional Neural Networks\\Data set, Presentation and Python Notebook for Image recognition project module\\train'\n",
    "\n",
    "# Define the classes\n",
    "classes = ['cats', 'dogs']\n",
    "\n",
    "# Define the target size of the images\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Loop through the folders in the data directory\n",
    "for class_name in classes:\n",
    "    class_dir = os.path.join(data_dir, class_name)\n",
    "    for filename in os.listdir(class_dir):\n",
    "        if filename.endswith('.jpg'):  # check if the file is an image\n",
    "            # Load the image and resize it\n",
    "            img = Image.open(os.path.join(class_dir, filename)).resize(target_size)\n",
    "            images.append(np.array(img))\n",
    "            if class_name == 'cats':\n",
    "                labels.append(0)\n",
    "            elif class_name == 'dogs':\n",
    "                labels.append(1)\n",
    "\n",
    "# Convert the image and label lists to numpy arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images,labels ,test_size=0.2,random_state=42)\n",
    "y_train =y_train.reshape(1,-1)\n",
    "y_test=y_test.reshape(1,-1)\n",
    "y_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlz34bZftw35",
    "outputId": "197aac3a-d96b-417a-d583-ba523b8d1e80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150528, 400)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_flatten = X_train.reshape(X_train.shape[0], -1).T   # \"-1\"  reshape flatten the remaining dimensions\n",
    "test_x_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "X_train_n = train_x_flatten/255.\n",
    "X_test_n = test_x_flatten/255.\n",
    "# print (\"train_x's shape: \" + str(X_train.shape))\n",
    "# print (\"test_x's shape: \" + str(X_test.shape))\n",
    "# test_x_flatten\n",
    "# train_x_flatten.shape\n",
    "X_train_n.shape\n",
    "X_test_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "ei3V8NCazQKA"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def softmax(z):\n",
    "    expZ = np.exp(z)\n",
    "    return expZ/(np.sum(expZ, 0))\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A\n",
    "def tanh(x):\n",
    "  return np.tanh(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuYpJMzg0-bA",
    "outputId": "16ee15cd-b30a-498d-c281-6462ffb8f9c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 400)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3eFvT2dazb4g",
    "outputId": "4e8d24f7-e2c7-4606-91a4-0fb5aa154a9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1600)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1lKQjDMz1BQ1",
    "outputId": "5a5239b5-d3c9-4492-b17a-c3cfcdd7163e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "ESGUgPPy1qkr"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \n",
    "    parameters = {} \n",
    "    L = len(layer_dims)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GXFG9ns5F0q",
    "outputId": "003052d0-4ac9-44d5-edb2-487decd01ab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W1: (100, 150528)\n",
      "Shape of B1: (100, 1) \n",
      "\n",
      "Shape of W2: (200, 100)\n",
      "Shape of B2: (200, 1) \n",
      "\n",
      "Shape of W3: (1, 200)\n",
      "Shape of B3: (1, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [X_train_n.shape[0], 100, 200, 1]\n",
    "params = initialize_parameters(layer_dims)\n",
    "\n",
    "for l in range(1, len(layer_dims)):\n",
    "    print(\"Shape of W\" + str(l) + \":\", params['W' + str(l)].shape)\n",
    "    print(\"Shape of B\" + str(l) + \":\", params['b' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "y-TWJMNh5Jik"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation):\n",
    "   \n",
    "    forward_cache = {}\n",
    "    L = len(parameters) // 2                  \n",
    "    \n",
    "    forward_cache['A0'] = X\n",
    "\n",
    "    for l in range(1, L):\n",
    "        forward_cache['Z' + str(l)] = parameters['W' + str(l)].dot(forward_cache['A' + str(l-1)]) + parameters['b' + str(l)]\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            forward_cache['A' + str(l)] = tanh(forward_cache['Z' + str(l)])\n",
    "        else:\n",
    "            forward_cache['A' + str(l)] = relu(forward_cache['Z' + str(l)])\n",
    "            \n",
    "\n",
    "    forward_cache['Z' + str(L)] = parameters['W' + str(L)].dot(forward_cache['A' + str(L-1)]) + parameters['b' + str(L)]\n",
    "    \n",
    "    if forward_cache['Z' + str(L)].shape[0] == 1:\n",
    "        forward_cache['A' + str(L)] = sigmoid(forward_cache['Z' + str(L)])\n",
    "    else :\n",
    "        forward_cache['A' + str(L)] = softmax(forward_cache['Z' + str(L)])\n",
    "\n",
    "   \n",
    "    \n",
    "    return forward_cache['A' + str(L)], forward_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "w4WLld7NI5yf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A0 : (150528, 1600)\n",
      "Shape of A1 : (100, 1600)\n",
      "Shape of A2 : (200, 1600)\n",
      "Shape of A3 : (1, 1600)\n"
     ]
    }
   ],
   "source": [
    "aL, forw_cache = forward_propagation(X_train_n, params, 'relu')\n",
    "\n",
    "for l in range(len(params)//2 + 1):\n",
    "    print(\"Shape of A\" + str(l) + \" :\", forw_cache['A' + str(l)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "wfuRbPC_wC4x"
   },
   "outputs": [],
   "source": [
    "def derivative_relu(Z):\n",
    "    return np.array(Z > 0, dtype = 'float')\n",
    "\n",
    "def derivative_tanh(x):\n",
    "    return (1 - np.power(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "vGBH7e9Fm0yV"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    \n",
    "    if Y.shape[0] == 1:\n",
    "        cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    else:\n",
    "        cost = -(1./m) * np.sum(Y * np.log(AL))\n",
    "        \n",
    "    cost = np.squeeze(cost)   \n",
    "    print(cost)   # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "QuKNx3u4m4YC"
   },
   "outputs": [],
   "source": [
    "def backward_propagation(AL, Y, parameters, forward_cache, activation):\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(parameters)//2\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    grads[\"dZ\" + str(L)] = AL - Y\n",
    "    grads[\"dW\" + str(L)] = 1./m * np.dot(grads[\"dZ\" + str(L)],forward_cache['A' + str(L-1)].T)\n",
    "    grads[\"db\" + str(L)] = 1./m * np.sum(grads[\"dZ\" + str(L)], axis = 1, keepdims = True)\n",
    "    \n",
    "    for l in reversed(range(1, L)):\n",
    "        if activation == 'tanh':\n",
    "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_tanh(forward_cache['A' + str(l)])\n",
    "        else:\n",
    "            grads[\"dZ\" + str(l)] = np.dot(parameters['W' + str(l+1)].T,grads[\"dZ\" + str(l+1)])*derivative_relu(forward_cache['A' + str(l)])\n",
    "            \n",
    "        grads[\"dW\" + str(l)] = 1./m * np.dot(grads[\"dZ\" + str(l)],forward_cache['A' + str(l-1)].T)\n",
    "        grads[\"db\" + str(l)] = 1./m * np.sum(grads[\"dZ\" + str(l)], axis = 1, keepdims = True)\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "zOu7nKswnBnD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dZ3 : (1, 1600)\n",
      "Shape of dW3 : (1, 200)\n",
      "Shape of dB3 : (1, 1) \n",
      "\n",
      "Shape of dZ2 : (200, 1600)\n",
      "Shape of dW2 : (200, 100)\n",
      "Shape of dB2 : (200, 1) \n",
      "\n",
      "Shape of dZ1 : (100, 1600)\n",
      "Shape of dW1 : (100, 150528)\n",
      "Shape of dB1 : (100, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grads = backward_propagation(forw_cache[\"A\" + str(3)], y_train, params, forw_cache, 'relu')\n",
    "\n",
    "for l in reversed(range(1, len(grads)//3 + 1)):\n",
    "    print(\"Shape of dZ\" + str(l) + \" :\", grads['dZ' + str(l)].shape)\n",
    "    print(\"Shape of dW\" + str(l) + \" :\", grads['dW' + str(l)].shape)\n",
    "    print(\"Shape of dB\" + str(l) + \" :\", grads['db' + str(l)].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "hYBTZ8oTnDKz"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "\n",
    "    L = len(parameters) // 2 \n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "E3J6GQWpnH7A"
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parameters, activation):\n",
    "\n",
    "    m = X.shape[1]\n",
    "    y_pred, caches = forward_propagation(X, parameters, activation)\n",
    "    \n",
    "    if y.shape[0] == 1:\n",
    "        y_pred = np.array(y_pred > 0.5, dtype = 'float')\n",
    "    else:\n",
    "        y = np.argmax(y, 0)\n",
    "        y_pred = np.argmax(y_pred, 0)\n",
    "    \n",
    "    return np.round(np.sum((y_pred == y)/m), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "PtrmJ1bRnSiE"
   },
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.03, activation = 'relu', num_iterations = 3000):#lr was 0.009\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []              \n",
    "    \n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        AL, forward_cache = forward_propagation(X, parameters, activation)\n",
    "        print(Y.shape)\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "\n",
    "        grads = backward_propagation(AL, Y, parameters, forward_cache, activation)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if i % (num_iterations/10) == 0:\n",
    "            print(\"\\niter:{} \\t cost: {} \\t train_acc:{} \\t test_acc:{}\".format(i, np.round(cost, 2), predict(X_train_n, y_train, parameters, activation), predict(X_test_n, y_test, parameters, activation)))\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"==\", end = '')\n",
    "\n",
    "       \n",
    "       \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "u56PNS9Hn6Jb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1600)\n",
      "1114.6610676283317\n",
      "\n",
      "iter:0 \t cost: 1114.66 \t train_acc:0.5 \t test_acc:0.5\n",
      "==(1, 1600)\n",
      "1137.549281803495\n",
      "(1, 1600)\n",
      "1123.5454571995115\n",
      "(1, 1600)\n",
      "1106.392593671993\n",
      "(1, 1600)\n",
      "1104.4893655527098\n",
      "(1, 1600)\n",
      "1110.2272428741971\n",
      "(1, 1600)\n",
      "1102.2589983476537\n",
      "(1, 1600)\n",
      "1098.5685341512603\n",
      "(1, 1600)\n",
      "1095.7006662821839\n",
      "(1, 1600)\n",
      "1093.6231839408138\n",
      "(1, 1600)\n",
      "1093.854068615522\n",
      "==(1, 1600)\n",
      "1099.8198026033779\n",
      "(1, 1600)\n",
      "1142.7024403691184\n",
      "(1, 1600)\n",
      "1110.702092457262\n",
      "(1, 1600)\n",
      "1102.3991908159737\n",
      "(1, 1600)\n",
      "1099.6132553858351\n",
      "(1, 1600)\n",
      "1098.3645659571534\n",
      "(1, 1600)\n",
      "1102.9884607452968\n",
      "(1, 1600)\n",
      "1112.2357383755361\n",
      "(1, 1600)\n",
      "1105.2314826384338\n",
      "(1, 1600)\n",
      "1099.0134757514268\n",
      "\n",
      "iter:20 \t cost: 1099.01 \t train_acc:0.57 \t test_acc:0.56\n",
      "==(1, 1600)\n",
      "1095.3023353834537\n",
      "(1, 1600)\n",
      "1090.8890558829125\n",
      "(1, 1600)\n",
      "1088.673487017592\n",
      "(1, 1600)\n",
      "1091.6621414798742\n",
      "(1, 1600)\n",
      "1118.0405304429103\n",
      "(1, 1600)\n",
      "1102.0316259945525\n",
      "(1, 1600)\n",
      "1088.1651150073637\n",
      "(1, 1600)\n",
      "1091.156366131175\n",
      "(1, 1600)\n",
      "1107.1671225137177\n",
      "(1, 1600)\n",
      "1096.330895886022\n",
      "==(1, 1600)\n",
      "1088.8453182167862\n",
      "(1, 1600)\n",
      "1084.3333880978184\n",
      "(1, 1600)\n",
      "1077.7791179015599\n",
      "(1, 1600)\n",
      "1095.1455707696791\n",
      "(1, 1600)\n",
      "1142.6375541545015\n",
      "(1, 1600)\n",
      "1100.469327218797\n",
      "(1, 1600)\n",
      "1093.8988048626281\n",
      "(1, 1600)\n",
      "1088.806490721789\n",
      "(1, 1600)\n",
      "1097.357178214802\n",
      "(1, 1600)\n",
      "1091.069768250889\n",
      "\n",
      "iter:40 \t cost: 1091.07 \t train_acc:0.58 \t test_acc:0.56\n",
      "==(1, 1600)\n",
      "1077.9227388684178\n",
      "(1, 1600)\n",
      "1089.1161689299647\n",
      "(1, 1600)\n",
      "1117.1125078200994\n",
      "(1, 1600)\n",
      "1115.7830210129514\n",
      "(1, 1600)\n",
      "1093.8876338093664\n",
      "(1, 1600)\n",
      "1086.210223453009\n",
      "(1, 1600)\n",
      "1080.5485812241475\n",
      "(1, 1600)\n",
      "1075.805539203001\n",
      "(1, 1600)\n",
      "1072.061623704521\n",
      "(1, 1600)\n",
      "1069.4221220349314\n",
      "==(1, 1600)\n",
      "1067.398223454293\n",
      "(1, 1600)\n",
      "1066.082312157124\n",
      "(1, 1600)\n",
      "1067.145812485926\n",
      "(1, 1600)\n",
      "1080.917563631762\n",
      "(1, 1600)\n",
      "1108.8440870684076\n",
      "(1, 1600)\n",
      "1078.904282873319\n",
      "(1, 1600)\n",
      "1072.9566356951696\n",
      "(1, 1600)\n",
      "1067.2744255591958\n",
      "(1, 1600)\n",
      "1066.4431599523596\n",
      "(1, 1600)\n",
      "1066.022023090325\n",
      "\n",
      "iter:60 \t cost: 1066.02 \t train_acc:0.58 \t test_acc:0.56\n",
      "==(1, 1600)\n",
      "1071.1770720005297\n",
      "(1, 1600)\n",
      "1066.0109362702208\n",
      "(1, 1600)\n",
      "1071.669119194806\n",
      "(1, 1600)\n",
      "1070.3511364169653\n",
      "(1, 1600)\n",
      "1069.5447931797098\n",
      "(1, 1600)\n",
      "1061.7043819679034\n",
      "(1, 1600)\n",
      "1079.406310711628\n",
      "(1, 1600)\n",
      "1087.119074535458\n",
      "(1, 1600)\n",
      "1078.6391898198515\n",
      "(1, 1600)\n",
      "1067.5247248454052\n",
      "==(1, 1600)\n",
      "1063.0367933310054\n",
      "(1, 1600)\n",
      "1053.9127705368446\n",
      "(1, 1600)\n",
      "1059.628522253262\n",
      "(1, 1600)\n",
      "1073.948921223697\n",
      "(1, 1600)\n",
      "1077.437752439846\n",
      "(1, 1600)\n",
      "1057.5698755776639\n",
      "(1, 1600)\n",
      "1046.9255904977856\n",
      "(1, 1600)\n",
      "1043.4942480824193\n",
      "(1, 1600)\n",
      "1048.506926913885\n",
      "(1, 1600)\n",
      "1076.2055102696745\n",
      "\n",
      "iter:80 \t cost: 1076.21 \t train_acc:0.56 \t test_acc:0.53\n",
      "==(1, 1600)\n",
      "1087.1761553978843\n",
      "(1, 1600)\n",
      "1079.5087125337643\n",
      "(1, 1600)\n",
      "1070.4524939102266\n",
      "(1, 1600)\n",
      "1051.673339734941\n",
      "(1, 1600)\n",
      "1045.0414794476637\n",
      "(1, 1600)\n",
      "1039.5067590688288\n",
      "(1, 1600)\n",
      "1033.7348710739743\n",
      "(1, 1600)\n",
      "1029.738742334813\n",
      "(1, 1600)\n",
      "1026.8741078347698\n",
      "(1, 1600)\n",
      "1026.1669310975449\n",
      "==(1, 1600)\n",
      "1043.5986461311181\n",
      "(1, 1600)\n",
      "1110.8046257767783\n",
      "(1, 1600)\n",
      "1123.0815668655307\n",
      "(1, 1600)\n",
      "1057.2036830019306\n",
      "(1, 1600)\n",
      "1049.930224096826\n",
      "(1, 1600)\n",
      "1045.4564824279046\n",
      "(1, 1600)\n",
      "1043.2455729635005\n",
      "(1, 1600)\n",
      "1045.1435548436737\n",
      "(1, 1600)\n",
      "1062.4350873313583\n",
      "(1, 1600)\n",
      "1096.1630226327818\n",
      "\n",
      "iter:100 \t cost: 1096.16 \t train_acc:0.55 \t test_acc:0.52\n",
      "==(1, 1600)\n",
      "1094.9517706004413\n",
      "(1, 1600)\n",
      "1055.013983931187\n",
      "(1, 1600)\n",
      "1041.9523046028428\n",
      "(1, 1600)\n",
      "1032.6150891786924\n",
      "(1, 1600)\n",
      "1027.4337479240908\n",
      "(1, 1600)\n",
      "1029.4418213919214\n",
      "(1, 1600)\n",
      "1042.0187969195067\n",
      "(1, 1600)\n",
      "1065.9471840343513\n",
      "(1, 1600)\n",
      "1078.7237185271879\n",
      "(1, 1600)\n",
      "1056.118087476908\n",
      "==(1, 1600)\n",
      "1042.314937229536\n",
      "(1, 1600)\n",
      "1020.8384935965789\n",
      "(1, 1600)\n",
      "1020.6658413418234\n",
      "(1, 1600)\n",
      "1041.093168665665\n",
      "(1, 1600)\n",
      "1076.1670168670971\n",
      "(1, 1600)\n",
      "1074.4961978552974\n",
      "(1, 1600)\n",
      "1060.288200493224\n",
      "(1, 1600)\n",
      "1024.1159113398035\n",
      "(1, 1600)\n",
      "1016.6943670431547\n",
      "(1, 1600)\n",
      "1011.2159464272706\n",
      "\n",
      "iter:120 \t cost: 1011.22 \t train_acc:0.66 \t test_acc:0.62\n",
      "==(1, 1600)\n",
      "1006.685354531329\n",
      "(1, 1600)\n",
      "1002.8845361930821\n",
      "(1, 1600)\n",
      "1007.0231961721556\n",
      "(1, 1600)\n",
      "1070.3228374720336\n",
      "(1, 1600)\n",
      "1159.9794103611002\n",
      "(1, 1600)\n",
      "1044.478414643038\n",
      "(1, 1600)\n",
      "1037.2809177775557\n",
      "(1, 1600)\n",
      "1036.016662855039\n",
      "(1, 1600)\n",
      "1044.9184361468547\n",
      "(1, 1600)\n",
      "1063.6121575706193\n",
      "==(1, 1600)\n",
      "1097.7162522085237\n",
      "(1, 1600)\n",
      "1038.7050954116614\n",
      "(1, 1600)\n",
      "1033.426100511213\n",
      "(1, 1600)\n",
      "1029.1963717076392\n",
      "(1, 1600)\n",
      "1037.4028084525748\n",
      "(1, 1600)\n",
      "1039.0292879005433\n",
      "(1, 1600)\n",
      "1059.6779616933982\n",
      "(1, 1600)\n",
      "1035.6142026160683\n",
      "(1, 1600)\n",
      "1035.4433903255067\n",
      "(1, 1600)\n",
      "1025.6042507011098\n",
      "\n",
      "iter:140 \t cost: 1025.6 \t train_acc:0.62 \t test_acc:0.59\n",
      "==(1, 1600)\n",
      "1033.0006006125104\n",
      "(1, 1600)\n",
      "1034.670486764362\n",
      "(1, 1600)\n",
      "1057.3340070944964\n",
      "(1, 1600)\n",
      "1040.6041825419572\n",
      "(1, 1600)\n",
      "1039.230387631617\n",
      "(1, 1600)\n",
      "1027.9021482285057\n",
      "(1, 1600)\n",
      "1032.1711692711087\n",
      "(1, 1600)\n",
      "1023.3401733810624\n",
      "(1, 1600)\n",
      "1033.1625849840577\n",
      "(1, 1600)\n",
      "1032.3014121970914\n",
      "==(1, 1600)\n",
      "1049.4583685351772\n",
      "(1, 1600)\n",
      "1021.6827556229904\n",
      "(1, 1600)\n",
      "1019.4663532952974\n",
      "(1, 1600)\n",
      "1014.9525442020313\n",
      "(1, 1600)\n",
      "1027.366452095754\n",
      "(1, 1600)\n",
      "1028.0793210985682\n",
      "(1, 1600)\n",
      "1041.734937568022\n",
      "(1, 1600)\n",
      "1016.808075536368\n",
      "(1, 1600)\n",
      "1018.0002551416607\n",
      "(1, 1600)\n",
      "1012.2082919612897\n",
      "\n",
      "iter:160 \t cost: 1012.21 \t train_acc:0.62 \t test_acc:0.58\n",
      "==(1, 1600)\n",
      "1026.4951484562434\n",
      "(1, 1600)\n",
      "1030.283995003595\n",
      "(1, 1600)\n",
      "1049.7979357177278\n",
      "(1, 1600)\n",
      "1011.9907217771437\n",
      "(1, 1600)\n",
      "1006.5274192512262\n",
      "(1, 1600)\n",
      "1000.314414534698\n",
      "(1, 1600)\n",
      "1013.2467656929243\n",
      "(1, 1600)\n",
      "1027.010517230797\n",
      "(1, 1600)\n",
      "1059.7077218066963\n",
      "(1, 1600)\n",
      "1008.3669447787424\n",
      "==(1, 1600)\n",
      "999.2406895377298\n",
      "(1, 1600)\n",
      "991.5078892062736\n",
      "(1, 1600)\n",
      "998.2017027533454\n",
      "(1, 1600)\n",
      "1016.509066839236\n",
      "(1, 1600)\n",
      "1063.020133553865\n",
      "(1, 1600)\n",
      "1014.619929972327\n",
      "(1, 1600)\n",
      "1010.8040950789341\n",
      "(1, 1600)\n",
      "993.1522219923734\n",
      "(1, 1600)\n",
      "996.448980292849\n",
      "(1, 1600)\n",
      "1002.6212261016981\n",
      "\n",
      "iter:180 \t cost: 1002.62 \t train_acc:0.6 \t test_acc:0.56\n",
      "==(1, 1600)\n",
      "1040.1832757121772\n",
      "(1, 1600)\n",
      "1022.0688839206173\n",
      "(1, 1600)\n",
      "1027.9560051596284\n",
      "(1, 1600)\n",
      "991.0482916330652\n",
      "(1, 1600)\n",
      "987.6544731965205\n",
      "(1, 1600)\n",
      "984.2571558748099\n",
      "(1, 1600)\n",
      "1003.7047744156459\n",
      "(1, 1600)\n",
      "1029.4411477436715\n",
      "(1, 1600)\n",
      "1079.2182552166455\n",
      "(1, 1600)\n",
      "1001.6533829196235\n",
      "==(1, 1600)\n",
      "987.8866969699948\n",
      "(1, 1600)\n",
      "978.8161276836825\n",
      "(1, 1600)\n",
      "977.3641134880029\n",
      "(1, 1600)\n",
      "980.8964450678614\n",
      "(1, 1600)\n",
      "1012.5275749251799\n",
      "(1, 1600)\n",
      "1041.346833343654\n",
      "(1, 1600)\n",
      "1081.6940317395329\n",
      "(1, 1600)\n",
      "998.9739309566748\n",
      "(1, 1600)\n",
      "987.7085221190248\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [X_train_n.shape[0], 100,100, y_train.shape[0]] #  4-layer model\n",
    "lr = 0.0075\n",
    "iters = 200\n",
    "\n",
    "parameters = model(X_train_n, y_train, layers_dims, learning_rate = lr, activation = 'relu', num_iterations = iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-dcXql5cDA9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
